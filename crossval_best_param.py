# -*- coding: utf-8 -*-
"""CrossVal_best_param.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FZXesmEyoSj3bk_I_RRpr5wp-9FdROO2
"""

from google.colab import drive

drive.mount('/content/drive')

print("HOPE")
import warnings
warnings.filterwarnings("ignore")
#to supress warnings

#make a function to unpickle
def unpickle(file):
    import pickle
    with open(file, 'rb') as fo:
        dict = pickle.load(fo, encoding='bytes')
    return dict
data_gray = unpickle('/content/drive/My Drive/ML/data5.pickle')

#After data, Take only 5 classes and having 200 samples of each classes
import numpy as np
print(data_gray.keys())
x_train = data_gray['x_train']
y_train = data_gray['y_train']
#Now seperate out classes and downsample it :)
c = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
#Combine the result of Xtrain and Xval
X = x_train 
Y = y_train 
X = np.array(X)
Y = np.array(Y)
X_test = np.array(data_gray['x_test'])
Y_test = np.array(data_gray['y_test'])
filx = []
fily = [] #filtered X and Y
print(X.shape,"shape")
print(len(X), "len")
# k = []

# from skimage import data, io, filters
# ex_train = []
# ex_test = []
# for i in range(len(X)):
#   edges = filters.sobel(X[i][0])
#   ex_train.append(edges)
  
# #for testing data 
# for i in range(len(X_test)):
#   edges = filters.sobel(X_test[i][0])
#   ex_test.append(edges)
# ex_train=np.array(ex_train)
# ex_test=np.array(ex_test)
# print(ex_train.shape)
# print(ex_test.shape)


ls = [0,14,18,23,33]
size = 500

for i in range(len(X)):
  #get the corresponding class y = 0, 14, 18, 23, 33
  jk = y_train[i]
  if(c[jk]<size):
    filx.append(X[i])
    fily.append(Y[i])
    c[jk]+=1
print(np.array(filx).shape, "fil shape")
print(fily)

Training_X = []
for i in range(len(filx)):
  a = filx[i].flatten()
  Training_X.append(a)
Training_X = np.array(Training_X)
print(Training_X.shape)

# #This would be used for tuning gamma and C for SVM
# from sklearn import svm
# from sklearn.model_selection import GridSearchCV
# def svc_param_selection(X, y, nfolds):
#     Cs = [0.001, 0.01, 0.1, 1, 10]
#     gammas = [0.001, 0.01, 0.1, 1]
#     param_grid = {'C': Cs, 'gamma' : gammas}
#     grid_search = GridSearchCV(svm.SVC(kernel='rbf'), param_grid, cv=nfolds, verbose=100)
#     grid_search.fit(X, y)
#     grid_search.best_params_
#     return grid_search.best_params_

# bestparams = svc_param_selection(Training_X,fily,3)
# print(bestparams)

from sklearn.svm import SVC
from sklearn import metrics
Training_X_full = []
for i in range(len(filx)):
  a = filx[i].flatten()
  Training_X_full.append(a)

print("after something")
Training_X_full = np.array(Training_X_full)
print(Training_X_full.shape)
instance = SVC(kernel='rbf',gamma= 'scale', verbose=1000)
instance.fit(Training_X_full,fily)
print("af fitting")
test = []
for i in range(len(X_test)):
  a = X_test[i].flatten()
  test.append(a)
y_pred = instance.predict(test)
accuracy = metrics.accuracy_score(Y_test,y_pred)
print('Accuracy: {:.2f}'.format(accuracy))

count_misclassified = (Y_test != y_pred).sum()
print(count_misclassified)









