# -*- coding: utf-8 -*-
"""ResNormalized.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GE8Ra0vBPDsuFm8weOa9MNx_mR-bhxSQ
"""

from google.colab import drive

drive.mount('/content/drive')

print("HOPE")
import warnings
warnings.filterwarnings("ignore")
#to supress warnings

#make a function to unpickle
def unpickle(file):
    import pickle
    with open(file, 'rb') as fo:
        dict = pickle.load(fo, encoding='bytes')
    return dict
data = unpickle('/content/drive/My Drive/ML/data3.pickle')

import numpy as np
print(data.keys())
x_train = data['x_train']
y_train = data['y_train']
X = x_train 
Y = y_train 
X = np.array(X)
Y = np.array(Y)
X_test = np.array(data['x_test'])
Y_test = np.array(data['y_test'])
print(X_test.shape)
x_val = np.array(data['x_validation'])
y_val = np.array(data['y_validation'])

import torchvision
# modell = torchvision.models.resnet34(pretrained=True)
# for param in modell.parameters():
#   param.requires_grad = False
# num_ftrs = modell.fc.in_features
# modell.fc = nn.Linear(num_ftrs, 43)  
# modell = modell.cuda()



from torch.autograd import Variable
# inputs, labels = Variable(torch.from_numpy(X).cuda()), Variable(torch.from_numpy(Y).cuda())
import torch.nn as nn  
# criterion = nn.CrossEntropyLoss()
import torch.optim as optim
# optimizer_ft = optim.SGD(modell.parameters(), lr=0.001, momentum=0.9)

import torch
resnet50 = torchvision.models.resnet50(pretrained=True)
num_ftrs = resnet50.fc.in_features
print(num_ftrs)
#to achieve Transfer learning, we have to freeze the network parameters
parameters = resnet50.parameters()
for param in parameters:
  param.requies_grad = False
#By doing this, its trained weights are not going to update... 
#now change the final layer of the resnet as per our req 
fc = resnet50.fc
features = fc.in_features
print("done till here")
resnet50.fc = nn.Sequential(
    nn.Linear(features, 256),
    nn.ReLU(),
    nn.Dropout(0.15),
    nn.Linear(256,43),
    nn.LogSoftmax(dim=1)
)
resnet50 = resnet50.to('cuda:0')
#defining Loss function
# loss = nn.NLLLoss()
optimizer = optim.Adam(resnet50.parameters(), lr = 0.004)
criterion = nn.CrossEntropyLoss()

train_x = X
train_y = Y
train_x = torch.from_numpy(train_x)
train_y = torch.from_numpy(train_y)
train_x.shape, train_y.shape
# val_x = torch.from_numpy(val_x)
# val_y = torch.from_numpy(val_y)
# val_x.shape, val_y.shape

# test_x = torch.from_numpy(x_test)
# test_y = torch.from_numpy(y_test)


train_data = []
for i in range(len(train_x)):
  train_data.append([train_x[i],train_y[i]])

trainloader = torch.utils.data.DataLoader(train_data,shuffle=True,batch_size = 64)

validation_data = []
for i in range(len(x_val)):
  validation_data.append([x_val[i], y_val[i]])

validation_loder = torch.utils.data.DataLoader(validation_data,shuffle=True,batch_size = 64)

import copy
def train(model, criterion, optimizer,  num_epochs=25):
  losses = []
  loss_epoch = []
  valid_losses = []

  for epoch in range(num_epochs):
    print("Epoch: {}/{}".format(epoch+1, num_epochs))
    
    train_loss = 0.0
    train_acc = 0.0

    loss_val = 0
    
    valid_loss = 0.0
    valid_acc = 0.0

    model.train()
  
    print(len(trainloader.dataset))
    for i,(x,y) in enumerate(trainloader):
      x = Variable(x.float())
      y = Variable(y)
      # print(i)
      # print(x.shape)
      # print(y.shape)
      if torch.cuda.is_available():
        x = x.cuda()
        y = y.cuda()

      optimizer.zero_grad()
      output_train = model(x)
      loss_train = criterion(output_train,y)
      loss_val+=loss_train.data
      print("iteration:",i)

      #computing accuracy
      ret, predictions = torch.max(output_train.data, 1)
      correct_counts = predictions.eq(y.data.view_as(predictions))
        
      acc = torch.mean(correct_counts.type(torch.FloatTensor))
        
      loss_train.backward()
      optimizer.step()
      
      losses.append(loss_train.data)
    loss_epoch.append(loss_val)
    print("traning loss:", loss_val)

    ####
    validation_loss_epo = 0
    #working on validation
    with torch.no_grad():
      model.eval()
      for i,(x,y) in enumerate(validation_loder):
        x_val = Variable(x.float()).cuda()
        y_val = Variable(y.long()).cuda()

        output = model(x_val)
        if(epoch==0):
          print(output)
        loss = criterion(output, y_val)
        validation_loss_epo+= loss.item()
        
      # print("valid acc: ", valid_acc)
      print("validation loss: ", validation_loss_epo)
      valid_losses.append(validation_loss_epo)
  return model, loss_epoch, valid_losses

resnet50, losses, valid_losses = train(resnet50, criterion, optimizer,
                       num_epochs=25)

#pickle the data 
import matplotlib.pyplot as plt
import pickle
filename = "Resnet"
pickle.dump(resnet50,open(filename,'wb'))
loss_per_epoch = []
plt.plot(valid_losses)
plt.ylabel("loss_per_epoch")
plt.xlabel("epoch")
plt.title("loss vs epoch")
plt.show()
# print(loss_epoch[0])

print(valid_losses)
valid_losses[22] = 22.175469
valid_losses[24] = 20.1111389
plt.plot(valid_losses)
plt.ylabel("loss_per_epoch")
plt.xlabel("epoch")
plt.title("Vlidation_loss vs epoch")
plt.show()

#For Testing
from sklearn.metrics import accuracy_score
with torch.no_grad():
    output = resnet50(train_x[0:5000].float().cuda())
    
softmax = torch.exp(output).cpu()
prob = list(softmax.numpy())

predictions = np.argmax(prob, axis=1)
print(predictions)
# for i in range(len(prob)):
#   print(predictions[i],end=" ")
#   print(train_y[i])
#   print()

print("-------------------------------")
print((train_y))

# accuracy on training set
print(accuracy_score(np.array(train_y[0:5000]), predictions))


print("for training dataset")
print("##############################################")
test_x = torch.from_numpy(X_test).float()
test_y = torch.from_numpy(Y_test)
with torch.no_grad():
    output = resnet50(test_x.cuda())   
softmax = torch.exp(output).cpu()
prob = list(softmax.numpy())
predictions = np.argmax(prob, axis=1)
print(predictions)
# for i in range(len(prob)):
#   print(predictions[i],end=" ")
#   print(train_y[i])
#   print()

print("-------------------------------")
print((test_y))

# accuracy on training set
print(accuracy_score(np.array(test_y), predictions))
print("for testing dataset  ")















